{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary modules here\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    '''\n",
    "        the above mentioned bottleneck, including two conv layer, one's kernel size is 1×1, another's is 3×3\n",
    "\n",
    "        after non-linear operation, concatenate the input to the output\n",
    "    '''\n",
    "    def __init__(self, in_planes, growth_rate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        \n",
    "        # input and output are concatenated here\n",
    "        out = torch.cat([out,x], 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    '''\n",
    "        transition layer is used for down sampling the feature\n",
    "        \n",
    "        when compress rate is 0.5, out_planes is a half of in_planes\n",
    "    '''\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_planes)\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.conv(F.relu(self.bn(x)))\n",
    "        # use average pooling change the size of feature map here\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out \n",
    "\n",
    "    \n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        '''\n",
    "        Args:\n",
    "            block: bottleneck\n",
    "            nblock: a list, the elements is number of bottleneck in each denseblock\n",
    "            growth_rate: channel size of bottleneck's output\n",
    "            reduction: \n",
    "        '''\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        num_planes = 2*growth_rate\n",
    "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
    "        \n",
    "        # a DenseBlock and a transition layer\n",
    "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
    "        num_planes += nblocks[0]*growth_rate\n",
    "        # the channel size is superposed, mutiply by reduction to cut it down here, the reduction is also known as compress rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans1 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "        \n",
    "        # a DenseBlock and a transition layer\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1]*growth_rate\n",
    "        # the channel size is superposed, mutiply by reduction to cut it down here, the reduction is also known as compress rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans2 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        # a DenseBlock and a transition layer\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2]*growth_rate\n",
    "        # the channel size is superposed, mutiply by reduction to cut it down here, the reduction is also known as compress rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans3 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        # only one DenseBlock \n",
    "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
    "        num_planes += nblocks[3]*growth_rate\n",
    "\n",
    "        # the last part is a linear layer as a classifier\n",
    "        self.bn = nn.BatchNorm2d(num_planes)\n",
    "        self.linear = nn.Linear(num_planes, num_classes)\n",
    "\n",
    "    def _make_dense_layers(self, block, in_planes, nblock):\n",
    "        layers = []\n",
    "        \n",
    "        # number of non-linear transformations in one DenseBlock depends on the parameter you set\n",
    "        for i in range(nblock):\n",
    "            layers.append(block(in_planes, self.growth_rate))\n",
    "            in_planes += self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def densenet():\n",
    "    return DenseNet(Bottleneck, [2, 5, 4, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def train(epoch, model, lossFunction, optimizer, device, trainloader):\n",
    "    \"\"\"train model using loss_fn and optimizer. When this function is called, model trains for one epoch.\n",
    "    Args:\n",
    "        train_loader: train data\n",
    "        model: prediction model\n",
    "        loss_fn: loss function to judge the distance between target and outputs\n",
    "        optimizer: optimize the loss function\n",
    "        get_grad: True, False\n",
    "    output:\n",
    "        total_loss: loss\n",
    "        average_grad2: average grad for hidden 2 in this epoch\n",
    "        average_grad3: average grad for hidden 3 in this epoch\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    model.train()     # enter train mode\n",
    "    train_loss = 0    # accumulate every batch loss in a epoch\n",
    "    correct = 0       # count when model' prediction is correct i train set\n",
    "    total = 0         # total number of prediction in train set\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device) # load data to gpu device\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        optimizer.zero_grad()            # clear gradients of all optimized torch.Tensors'\n",
    "        outputs = model(inputs)          # forward propagation return the value of softmax function\n",
    "        loss = lossFunction(outputs, targets) #compute loss\n",
    "        loss.backward()                  # compute gradient of loss over parameters \n",
    "        optimizer.step()                 # update parameters with gradient descent \n",
    "\n",
    "        train_loss += loss.item()        # accumulate every batch loss in a epoch\n",
    "        _, predicted = outputs.max(1)    # make prediction according to the outputs\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item() # count how many predictions is correct\n",
    "        \n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            # print loss and acc\n",
    "            print( 'Train loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    print( 'Train loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    \n",
    "    \n",
    "def test(model, lossFunction, optimizer, device, testloader):\n",
    "    \"\"\"\n",
    "    test model's prediction performance on loader.  \n",
    "    When thid function is called, model is evaluated.\n",
    "    Args:\n",
    "        loader: data for evaluation\n",
    "        model: prediction model\n",
    "        loss_fn: loss function to judge the distance between target and outputs\n",
    "    output:\n",
    "        total_loss\n",
    "        accuracy\n",
    "    \"\"\"\n",
    "    global best_acc\n",
    "    model.eval() #enter test mode\n",
    "    test_loss = 0 # accumulate every batch loss in a epoch\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = lossFunction(outputs, targets) #compute loss\n",
    "\n",
    "            test_loss += loss.item() # accumulate every batch loss in a epoch\n",
    "            _, predicted = outputs.max(1) # make prediction according to the outputs\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item() # count how many predictions is correct\n",
    "        # print loss and acc\n",
    "        print('Test Loss: %.3f  | Test Acc: %.3f%% (%d/%d)'\n",
    "            % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "        \n",
    "def data_loader():\n",
    "    # define method of preprocessing data for evaluating\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize a tensor image with mean and standard variance\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize a tensor image with mean and standard variance\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    # prepare dataset by ImageFolder, data should be classified by directory\n",
    "    trainset = torchvision.datasets.ImageFolder(root='./household/train', transform=transform_train)\n",
    "\n",
    "    testset = torchvision.datasets.ImageFolder(root='./household/test', transform=transform_test)\n",
    "\n",
    "    # Data loader. \n",
    "\n",
    "    # Combines a dataset and a sampler, \n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
    "    return trainloader, testloader\n",
    "\n",
    "def run(model, num_epochs):\n",
    "    \n",
    "    # load model into GPU device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    if device == 'cuda':\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    # define the loss function and optimizer\n",
    "\n",
    "    lossFunction = nn.CrossEntropyLoss()\n",
    "    lr = 0.01\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    trainloader, testloader = data_loader()\n",
    "    for epoch in range(num_epochs):\n",
    "        train(epoch, model, lossFunction, optimizer, device, trainloader)\n",
    "        test(model, lossFunction, optimizer, device, testloader)\n",
    "        if (epoch + 1) % 50 == 0 :\n",
    "            lr = lr / 10\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet52():\n",
    "    return DenseNet(Bottleneck, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Train loss: 2.311 | Train Acc: 7.819% (19/243)\n",
      "Test Loss: 2.280  | Test Acc: 20.000% (10/50)\n",
      "\n",
      "Epoch: 1\n",
      "Train loss: 2.172 | Train Acc: 34.979% (85/243)\n",
      "Test Loss: 2.253  | Test Acc: 20.000% (10/50)\n",
      "\n",
      "Epoch: 2\n",
      "Train loss: 2.003 | Train Acc: 41.975% (102/243)\n",
      "Test Loss: 2.213  | Test Acc: 20.000% (10/50)\n",
      "\n",
      "Epoch: 3\n",
      "Train loss: 1.821 | Train Acc: 41.975% (102/243)\n",
      "Test Loss: 2.163  | Test Acc: 20.000% (10/50)\n",
      "\n",
      "Epoch: 4\n",
      "Train loss: 1.699 | Train Acc: 43.210% (105/243)\n",
      "Test Loss: 2.102  | Test Acc: 20.000% (10/50)\n",
      "\n",
      "Epoch: 5\n",
      "Train loss: 1.570 | Train Acc: 44.444% (108/243)\n",
      "Test Loss: 2.032  | Test Acc: 28.000% (14/50)\n",
      "\n",
      "Epoch: 6\n",
      "Train loss: 1.451 | Train Acc: 50.617% (123/243)\n",
      "Test Loss: 1.943  | Test Acc: 26.000% (13/50)\n",
      "\n",
      "Epoch: 7\n",
      "Train loss: 1.345 | Train Acc: 53.498% (130/243)\n",
      "Test Loss: 1.834  | Test Acc: 26.000% (13/50)\n",
      "\n",
      "Epoch: 8\n",
      "Train loss: 1.248 | Train Acc: 59.671% (145/243)\n",
      "Test Loss: 1.748  | Test Acc: 22.000% (11/50)\n",
      "\n",
      "Epoch: 9\n",
      "Train loss: 1.193 | Train Acc: 58.848% (143/243)\n",
      "Test Loss: 1.664  | Test Acc: 26.000% (13/50)\n",
      "\n",
      "Epoch: 10\n",
      "Train loss: 1.067 | Train Acc: 64.609% (157/243)\n",
      "Test Loss: 1.450  | Test Acc: 40.000% (20/50)\n",
      "\n",
      "Epoch: 11\n",
      "Train loss: 0.961 | Train Acc: 72.016% (175/243)\n",
      "Test Loss: 1.196  | Test Acc: 58.000% (29/50)\n",
      "\n",
      "Epoch: 12\n",
      "Train loss: 0.944 | Train Acc: 69.136% (168/243)\n",
      "Test Loss: 1.035  | Test Acc: 70.000% (35/50)\n",
      "\n",
      "Epoch: 13\n",
      "Train loss: 0.912 | Train Acc: 71.605% (174/243)\n",
      "Test Loss: 1.067  | Test Acc: 56.000% (28/50)\n",
      "\n",
      "Epoch: 14\n",
      "Train loss: 0.924 | Train Acc: 65.844% (160/243)\n",
      "Test Loss: 1.471  | Test Acc: 42.000% (21/50)\n",
      "\n",
      "Epoch: 15\n",
      "Train loss: 0.826 | Train Acc: 73.251% (178/243)\n",
      "Test Loss: 1.186  | Test Acc: 60.000% (30/50)\n",
      "\n",
      "Epoch: 16\n",
      "Train loss: 0.857 | Train Acc: 68.313% (166/243)\n",
      "Test Loss: 0.957  | Test Acc: 68.000% (34/50)\n",
      "\n",
      "Epoch: 17\n",
      "Train loss: 0.788 | Train Acc: 70.782% (172/243)\n",
      "Test Loss: 0.890  | Test Acc: 64.000% (32/50)\n",
      "\n",
      "Epoch: 18\n",
      "Train loss: 0.769 | Train Acc: 72.428% (176/243)\n",
      "Test Loss: 0.948  | Test Acc: 64.000% (32/50)\n",
      "\n",
      "Epoch: 19\n",
      "Train loss: 0.684 | Train Acc: 79.835% (194/243)\n",
      "Test Loss: 0.923  | Test Acc: 60.000% (30/50)\n",
      "\n",
      "Epoch: 20\n",
      "Train loss: 0.662 | Train Acc: 79.424% (193/243)\n",
      "Test Loss: 0.828  | Test Acc: 78.000% (39/50)\n",
      "\n",
      "Epoch: 21\n",
      "Train loss: 0.659 | Train Acc: 81.481% (198/243)\n",
      "Test Loss: 0.780  | Test Acc: 68.000% (34/50)\n",
      "\n",
      "Epoch: 22\n",
      "Train loss: 0.562 | Train Acc: 82.305% (200/243)\n",
      "Test Loss: 0.646  | Test Acc: 80.000% (40/50)\n",
      "\n",
      "Epoch: 23\n",
      "Train loss: 0.535 | Train Acc: 84.362% (205/243)\n",
      "Test Loss: 0.705  | Test Acc: 74.000% (37/50)\n",
      "\n",
      "Epoch: 24\n",
      "Train loss: 0.483 | Train Acc: 85.597% (208/243)\n",
      "Test Loss: 0.729  | Test Acc: 72.000% (36/50)\n",
      "\n",
      "Epoch: 25\n",
      "Train loss: 0.513 | Train Acc: 81.893% (199/243)\n",
      "Test Loss: 0.332  | Test Acc: 88.000% (44/50)\n",
      "\n",
      "Epoch: 26\n",
      "Train loss: 0.469 | Train Acc: 83.128% (202/243)\n",
      "Test Loss: 0.494  | Test Acc: 84.000% (42/50)\n",
      "\n",
      "Epoch: 27\n",
      "Train loss: 0.475 | Train Acc: 83.539% (203/243)\n",
      "Test Loss: 0.241  | Test Acc: 94.000% (47/50)\n",
      "\n",
      "Epoch: 28\n",
      "Train loss: 0.460 | Train Acc: 85.597% (208/243)\n",
      "Test Loss: 0.242  | Test Acc: 94.000% (47/50)\n",
      "\n",
      "Epoch: 29\n",
      "Train loss: 0.390 | Train Acc: 88.066% (214/243)\n",
      "Test Loss: 0.388  | Test Acc: 82.000% (41/50)\n",
      "\n",
      "Epoch: 30\n",
      "Train loss: 0.386 | Train Acc: 88.889% (216/243)\n",
      "Test Loss: 0.504  | Test Acc: 84.000% (42/50)\n",
      "\n",
      "Epoch: 31\n",
      "Train loss: 0.439 | Train Acc: 83.951% (204/243)\n",
      "Test Loss: 0.623  | Test Acc: 74.000% (37/50)\n",
      "\n",
      "Epoch: 32\n",
      "Train loss: 0.491 | Train Acc: 85.597% (208/243)\n",
      "Test Loss: 0.264  | Test Acc: 88.000% (44/50)\n",
      "\n",
      "Epoch: 33\n",
      "Train loss: 0.452 | Train Acc: 83.128% (202/243)\n",
      "Test Loss: 2.728  | Test Acc: 46.000% (23/50)\n",
      "\n",
      "Epoch: 34\n",
      "Train loss: 0.489 | Train Acc: 83.951% (204/243)\n",
      "Test Loss: 1.914  | Test Acc: 60.000% (30/50)\n",
      "\n",
      "Epoch: 35\n",
      "Train loss: 0.339 | Train Acc: 90.535% (220/243)\n",
      "Test Loss: 0.558  | Test Acc: 80.000% (40/50)\n",
      "\n",
      "Epoch: 36\n",
      "Train loss: 0.321 | Train Acc: 87.243% (212/243)\n",
      "Test Loss: 0.482  | Test Acc: 84.000% (42/50)\n",
      "\n",
      "Epoch: 37\n",
      "Train loss: 0.292 | Train Acc: 90.947% (221/243)\n",
      "Test Loss: 0.355  | Test Acc: 86.000% (43/50)\n",
      "\n",
      "Epoch: 38\n",
      "Train loss: 0.349 | Train Acc: 88.477% (215/243)\n",
      "Test Loss: 0.251  | Test Acc: 88.000% (44/50)\n",
      "\n",
      "Epoch: 39\n",
      "Train loss: 0.299 | Train Acc: 91.358% (222/243)\n",
      "Test Loss: 0.244  | Test Acc: 92.000% (46/50)\n",
      "\n",
      "Epoch: 40\n",
      "Train loss: 0.281 | Train Acc: 90.123% (219/243)\n",
      "Test Loss: 0.192  | Test Acc: 94.000% (47/50)\n",
      "\n",
      "Epoch: 41\n",
      "Train loss: 0.347 | Train Acc: 88.477% (215/243)\n",
      "Test Loss: 0.124  | Test Acc: 98.000% (49/50)\n",
      "\n",
      "Epoch: 42\n",
      "Train loss: 0.223 | Train Acc: 94.239% (229/243)\n",
      "Test Loss: 1.736  | Test Acc: 66.000% (33/50)\n",
      "\n",
      "Epoch: 43\n",
      "Train loss: 0.251 | Train Acc: 91.770% (223/243)\n",
      "Test Loss: 0.224  | Test Acc: 90.000% (45/50)\n",
      "\n",
      "Epoch: 44\n",
      "Train loss: 0.292 | Train Acc: 91.358% (222/243)\n",
      "Test Loss: 1.428  | Test Acc: 64.000% (32/50)\n",
      "\n",
      "Epoch: 45\n",
      "Train loss: 0.239 | Train Acc: 93.004% (226/243)\n",
      "Test Loss: 0.752  | Test Acc: 72.000% (36/50)\n",
      "\n",
      "Epoch: 46\n",
      "Train loss: 0.257 | Train Acc: 89.712% (218/243)\n",
      "Test Loss: 0.263  | Test Acc: 92.000% (46/50)\n",
      "\n",
      "Epoch: 47\n",
      "Train loss: 0.238 | Train Acc: 93.416% (227/243)\n",
      "Test Loss: 0.169  | Test Acc: 94.000% (47/50)\n",
      "\n",
      "Epoch: 48\n",
      "Train loss: 0.280 | Train Acc: 91.770% (223/243)\n",
      "Test Loss: 0.108  | Test Acc: 98.000% (49/50)\n",
      "\n",
      "Epoch: 49\n",
      "Train loss: 0.233 | Train Acc: 92.593% (225/243)\n",
      "Test Loss: 0.240  | Test Acc: 92.000% (46/50)\n"
     ]
    }
   ],
   "source": [
    "# start training and testing\n",
    "model = densenet()\n",
    "# num_epochs is adjustable\n",
    "run(model, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "save_path = './model/classifier.pth'\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fan\n",
      "air-conditioning\n"
     ]
    }
   ],
   "source": [
    "# Model class must be defined somewhere\n",
    "model_object = densenet()\n",
    "model_object.load_state_dict(torch.load(save_path))\n",
    "model_object.eval()\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize a tensor image with mean and standard variance\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "myset = torchvision.datasets.ImageFolder(root='./household/my', transform=transform_test)\n",
    "\n",
    "myloader = torch.utils.data.DataLoader(myset, batch_size=1, shuffle=False)\n",
    "\n",
    "dic = {0:'air-conditioning', 1:'fan', 2:'loudspeaker-box', 3:'projector', 4:'television'}\n",
    "    \n",
    "with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(myloader):\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            inputs, targets = inputs, targets\n",
    "            model_object\n",
    "            outputs = model_object(inputs)\n",
    "            \n",
    "            _, predicted = outputs.max(1) # make prediction according to the outputs\n",
    "            print(dic[predicted.numpy()[0]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
